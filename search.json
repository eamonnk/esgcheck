[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ek-blog",
    "section": "",
    "text": "TU257 - Assignment B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/text_mining_eamonn_kelly_final.html#step-5---classification-models",
    "href": "posts/text_mining_eamonn_kelly_final.html#step-5---classification-models",
    "title": "TU257 - Assignment B",
    "section": "Step 5 - Classification Models",
    "text": "Step 5 - Classification Models\n5.1 - Split dataset into features and target labels\n\n# define good companies and bad companies#\n# sorting into positive and negative companies based off existing industry esg ratings\n# as have fata already extracted froim files will just assign exisint gdat based ont hen company key in the data\n# won't use load_files method as haver dat already extracted\n\ngood_companies = {\"kerrygroup\", \"crh\", \"smurfitkappa\", \"kingspan\"}\nbad_companies = {\"exxonmobil\", \"jbs\", 'aramco'}\n\n#contains processed cleaned report text from \nX_texts = []\ny_labels = []\n\n\n# assign positive y label to good companies\n# 1 is positive &gt; 0 is negative\n# we have two data set_matplotlib_closeprocessed_per_file_data = contains &gt; \"company\": company,  \"filename\": pdf_file.name,  \"text\": cleaned &gt;&gt;&gt; all data is separate dout per file and company key-value pairs\n# combined_per_comp_text = contains &gt; [record[\"company\"]] += \" \" + record[\"text\"] &gt;&gt;&gt; all data is combined across companies\n# we'll use the first as we have it separated out at file level giving us more elements to train with\nfor record in processed_per_file_data:\n    company = record['company'].lower()\n    text = record['text']\n    \n    \n    if company.lower() in good_companies:\n        X_texts.append(text)\n        y_labels.append(1)\n\n    elif company.lower() in bad_companies:\n        X_texts.append(text)\n        y_labels.append(0)\n    \n\n5.2 - Verify variables are as expoected\n\n# quick check tomake sure we're using the right variables and they are what we expect going into the models\nprint(\"Total documents:\", len(X_texts))  # Should be 43\nprint(\"Good ESG count:\", y_labels.count(1))  # Should be 33\nprint(\"Bad ESG count:\", y_labels.count(0))  # Should be 10\n\nTotal documents: 43\nGood ESG count: 33\nBad ESG count: 10\n\n\n5.3 - Vectorize the text\n\nvectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\n\n# Uses vectorization – takes all those words, getting into AI agent approach, rather than a single variable, converts variable into whole load of different dimensions\nX_counts = vectorizer.fit_transform(X_texts).toarray()\n\n\n5.4 - Apply tf-idf transformation\n\n# normalise the term frequency across the documents\ntfidfconverter = TfidfTransformer()\nX = tfidfconverter.fit_transform(X_counts).toarray()\n\n5.5 - Split data into train and test data and verify target variable data\n\n# spliut intotrain and test\nX_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.3, random_state=42, stratify=y_labels)\n\nprint(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))\nprint(len(X_train)/(len(X_train)+len(X_test)))\nprint(len(X_test)/(len(X_train)+len(X_test)))\n\npd.Series(y_train).value_counts().plot(kind='bar', title='Count (Pre-SMOTE - Target Variable in y_train dataset)')\n\n30\n30\n13\n13\n0.6976744186046512\n0.3023255813953488\n\n\n\n\n\n\n\n\n\n5.6 - Create Models, perform cross validation and plot ROC AUC\n\n# perform cross validation and plot the output\nclassifiers = [\n   ('NB', GaussianNB()),\n   ('DT', DecisionTreeClassifier()),\n   ('RF', RandomForestClassifier()),\n   ('LR', LogisticRegression()),\n   ('NN', nn.MLPClassifier()),\n   ('XGB', XGBClassifier(n_estimators=100)),\n   ('SVC', SVC(probability=True))]\n\n# Logging setup\nlog_cols = [\"Classifier\", \"Accuracy (CV Mean)\"]\nlog = pd.DataFrame(columns=log_cols)\nprint(log)\n\n# Plot ROC curve for K-fold, cross-validated predictions\nplt.clf()\nplt.figure(figsize=(12, 8))\n\n# Define the cross-validation strategy, we tried different numbers 3, 5, 10, 20 and 30,can't exceed the number of samples. The data set is small so it doesn't take too long. Set at 3 for now to keep runtime low for demo purposes.\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nprint(cv)\n\n# Loop through classifiers \nfor name, model in classifiers:\n    name = model.__class__.__name__\n\n    # Get cross validation prediction and probability\n    y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, method='predict')\n    y_pred_proba = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n\n    #  cross-validation accuracy scores and mean accuracy\n    cv_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    cv_mean = np.mean(cv_scores)\n    print(f\"Cross-validation accuracy for {name}: {cv_mean}\") \n   \n    # ROC curve and AUC for cross-validated model\n    ## here we using cross_val_predict : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html\n\n    #sklearn.model_selection.cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, \n    #params=None, pre_dispatch='2*n_jobs', method='predict')\n\n    ## basically getting model.predict_proba but for the cross validation figures. \n\n    #### Note we aren't comparing these directly, but just seeing if there are large differences between them more so. between cross validation \n    #### and the actual model evaluations below\n    ####### Note also this is all done on x_train and y_train. \n    \n    fpr_cv, tpr_cv, _ = roc_curve(y_train, y_pred_proba)\n    roc_auc_cv = auc(fpr_cv, tpr_cv)\n    print(f\"true pos and fal pos {fpr_cv} {tpr_cv}:\")\n\n    # Log results\n    log_entry = pd.DataFrame([[name, cv_mean]], columns=log_cols)\n    log = pd.concat([log, log_entry])\n   \n    # Plot ROC curves for cross-validation model\n    plt.plot(fpr_cv, tpr_cv, linestyle='--', label='%s CV ROC (area = %0.2f)' % (name, roc_auc_cv))\n\n    # Print confusion matrix for cross-validated predictions\n    cm_cv = confusion_matrix(y_train, y_pred_cv)\n    print(f\"Confusion Matrix for {name} (Cross-Validation):\")\n    print(cm_cv)\n    print('')\n\n\n# Finalize ROC plot\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Cross-Validation) - Pre-SMOTE Data - kfold nsplits = 5 ')\nplt.legend(loc=0, fontsize='small')\nplt.show()\n\n# Print log of results\nprint(log)\n\nEmpty DataFrame\nColumns: [Classifier, Accuracy (CV Mean)]\nIndex: []\nKFold(n_splits=5, random_state=42, shuffle=True)\nCross-validation accuracy for GaussianNB: 0.9333333333333333\ntrue pos and fal pos [0.         0.28571429 1.        ] [0. 1. 1.]:\nConfusion Matrix for GaussianNB (Cross-Validation):\n[[ 5  2]\n [ 0 23]]\n\nCross-validation accuracy for DecisionTreeClassifier: 0.7333333333333333\ntrue pos and fal pos [0.         0.85714286 1.        ] [0.         0.91304348 1.        ]:\nConfusion Matrix for DecisionTreeClassifier (Cross-Validation):\n[[ 2  5]\n [ 1 22]]\n\nCross-validation accuracy for RandomForestClassifier: 0.8333333333333334\ntrue pos and fal pos [0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.14285714 0.14285714 0.14285714\n 1.        ] [0.         0.04347826 0.13043478 0.2173913  0.30434783 0.47826087\n 0.60869565 0.69565217 0.73913043 0.7826087  0.91304348 1.\n 1.        ]:\nConfusion Matrix for RandomForestClassifier (Cross-Validation):\n[[ 1  6]\n [ 0 23]]\n\nCross-validation accuracy for LogisticRegression: 0.7666666666666666\ntrue pos and fal pos [0.         0.         0.         0.14285714 0.14285714 0.28571429\n 0.28571429 0.42857143 0.42857143 1.        ] [0.         0.04347826 0.7826087  0.7826087  0.86956522 0.86956522\n 0.95652174 0.95652174 1.         1.        ]:\nConfusion Matrix for LogisticRegression (Cross-Validation):\n[[ 0  7]\n [ 0 23]]\n\nCross-validation accuracy for MLPClassifier: 0.9666666666666668\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for MLPClassifier (Cross-Validation):\n[[ 6  1]\n [ 0 23]]\n\nCross-validation accuracy for XGBClassifier: 0.8\ntrue pos and fal pos [0.         0.         0.         0.         0.28571429 0.28571429\n 0.42857143 0.42857143 0.57142857 0.57142857 0.71428571 0.71428571\n 0.85714286 0.85714286 1.        ] [0.         0.04347826 0.13043478 0.30434783 0.47826087 0.69565217\n 0.69565217 0.7826087  0.7826087  0.82608696 0.82608696 0.95652174\n 0.95652174 1.         1.        ]:\nConfusion Matrix for XGBClassifier (Cross-Validation):\n[[ 1  6]\n [ 0 23]]\n\nCross-validation accuracy for SVC: 0.8333333333333334\ntrue pos and fal pos [0.         0.         0.         0.14285714 0.14285714 0.28571429\n 0.28571429 1.        ] [0.         0.04347826 0.82608696 0.82608696 0.95652174 0.95652174\n 1.         1.        ]:\nConfusion Matrix for SVC (Cross-Validation):\n[[ 2  5]\n [ 0 23]]\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n               Classifier  Accuracy (CV Mean)\n0              GaussianNB            0.933333\n0  DecisionTreeClassifier            0.733333\n0  RandomForestClassifier            0.833333\n0      LogisticRegression            0.766667\n0           MLPClassifier            0.966667\n0           XGBClassifier            0.800000\n0                     SVC            0.833333\n\n\n5.7 - Plot Accuracy data\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy (CV Mean)', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy Normal Models')\nplt.show()\n\n\n\n\n\n\n\n\n5.8 - Apply SMOTE to data to oversample the minority class and verify target variable data\n\n# ---SMOTE----\n\n\n# Apply SMOTE only to training data\nsm = SMOTE(random_state=42)\nX_train_ps, y_train_ps = sm.fit_resample(X_train, y_train)\n\nprint('SMOTE over-sampling:')\nprint('-----')\nprint('length of \\'X_train\\' = {}'.format(len(X_train_ps)))\nprint('length of \\'y_train\\' = {}'.format(len(y_train_ps)))\nprint('-----')\n\n\n#plot post SMOTE of just the target variable\n# y_train_ps['Target'].value_counts().plot(kind='bar', title='Count (Post-SMOTE - Target Variable in y_train dataset')\n# y_train_ps.value_counts().plot(kind='bar', title='Count (Post-SMOTE - Target Variable in y_train dataset)')\npd.Series(y_train_ps).value_counts().plot(kind='bar', title='Count (Post-SMOTE - Target Variable in y_train dataset)')\n\n# re-associate the '_ps' naming with the standard X_train and y_train naming, as it it used through modelling process below\nX_train=X_train_ps\ny_train=y_train_ps\n\nSMOTE over-sampling:\n-----\nlength of 'X_train' = 46\nlength of 'y_train' = 46\n-----\n\n\n\n\n\n\n\n\n\n5.9 - Create the Models using SMOTE data set\n\n# perform cross validation and plot the output\nclassifiers = [\n   ('NB', GaussianNB()),\n   ('DT', DecisionTreeClassifier()),\n   ('RF', RandomForestClassifier()),\n   ('LR', LogisticRegression()),\n   ('NN', nn.MLPClassifier()),\n   ('XGB', XGBClassifier(n_estimators=100)),\n   ('SVC', SVC(probability=True))]\n\n# Logging setup\nlog_cols = [\"Classifier\", \"Accuracy (CV Mean)\"]\nlog = pd.DataFrame(columns=log_cols)\nprint(log)\n\n# Plot ROC curve for K-fold, cross-validated predictions\nplt.clf()\nplt.figure(figsize=(12, 8))\n\n# Define the cross-validation strategy, we tried different numbers 3, 5, 10, 20 and 30,can't exceed the number of samples. 20 appeears to be close ot the sweet spot for number of kfolds. The data set is small so it doesn't take too long. Set at 3 for now to keep runtime low for demo purposes.\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nprint(cv)\n\n# Loop through classifiers \nfor name, model in classifiers:\n    name = model.__class__.__name__\n\n    # Get cross validation prediction and probability\n    y_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, method='predict')\n    y_pred_proba = cross_val_predict(model, X_train, y_train, cv=cv, method='predict_proba')[:, 1]\n\n    #  cross-validation accuracy scores and mean accuracy\n    cv_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    cv_mean = np.mean(cv_scores)\n    print(f\"Cross-validation accuracy for {name}: {cv_mean}\") \n   \n    # ROC curve and AUC for cross-validated model\n    ## here we using cross_val_predict : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html\n\n    #sklearn.model_selection.cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, \n    #params=None, pre_dispatch='2*n_jobs', method='predict')\n\n    ## basically getting model.predict_proba but for the cross validation figures. \n\n    #### Note we aren't comparing these directly, but just seeing if there are large differences between them more so. between cross validation \n    #### and the actual model evaluations below\n    ####### Note also this is all done on x_train and y_train. \n    \n    fpr_cv, tpr_cv, _ = roc_curve(y_train, y_pred_proba)\n    roc_auc_cv = auc(fpr_cv, tpr_cv)\n    print(f\"true pos and fal pos {fpr_cv} {tpr_cv}:\")\n\n    # Log results\n    log_entry = pd.DataFrame([[name, cv_mean]], columns=log_cols)\n    log = pd.concat([log, log_entry])\n   \n    # Plot ROC curves for cross-validation model\n    plt.plot(fpr_cv, tpr_cv, linestyle='--', label='%s CV ROC (area = %0.2f)' % (name, roc_auc_cv))\n\n    # Print confusion matrix for cross-validated predictions\n    cm_cv = confusion_matrix(y_train, y_pred_cv)\n    print(f\"Confusion Matrix for {name} (Cross-Validation):\")\n    print(cm_cv)\n    print('')\n\n\n# Finalize ROC plot\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Cross-Validation) - Post SMOTE Data - kfold nsplits = 5 ')\nplt.legend(loc=0, fontsize='small')\nplt.show()\n\n# Print log of results\nprint(log)\n\nEmpty DataFrame\nColumns: [Classifier, Accuracy (CV Mean)]\nIndex: []\nKFold(n_splits=5, random_state=42, shuffle=True)\nCross-validation accuracy for GaussianNB: 1.0\ntrue pos and fal pos [0. 0. 1.] [0. 1. 1.]:\nConfusion Matrix for GaussianNB (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\nCross-validation accuracy for DecisionTreeClassifier: 0.8022222222222222\ntrue pos and fal pos [0.         0.04347826 1.        ] [0.         0.69565217 1.        ]:\nConfusion Matrix for DecisionTreeClassifier (Cross-Validation):\n[[22  1]\n [ 6 17]]\n\nCross-validation accuracy for RandomForestClassifier: 1.0\ntrue pos and fal pos [0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.26086957 0.34782609 0.43478261\n 0.60869565 0.73913043 0.91304348 1.        ] [0.         0.04347826 0.13043478 0.2173913  0.56521739 0.65217391\n 0.69565217 0.7826087  1.         1.         1.         1.\n 1.         1.         1.         1.        ]:\nConfusion Matrix for RandomForestClassifier (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\nCross-validation accuracy for LogisticRegression: 1.0\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for LogisticRegression (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\nCross-validation accuracy for MLPClassifier: 0.9800000000000001\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for MLPClassifier (Cross-Validation):\n[[23  0]\n [ 1 22]]\n\nCross-validation accuracy for XGBClassifier: 0.8688888888888888\ntrue pos and fal pos [0.         0.         0.         0.         0.         0.\n 0.         0.04347826 0.04347826 0.08695652 0.08695652 0.13043478\n 0.13043478 0.17391304 0.26086957 0.43478261 0.65217391 0.7826087\n 0.82608696 0.91304348 1.        ] [0.         0.04347826 0.08695652 0.34782609 0.39130435 0.56521739\n 0.60869565 0.60869565 0.73913043 0.73913043 0.7826087  0.7826087\n 0.91304348 0.91304348 0.91304348 0.91304348 1.         1.\n 1.         1.         1.        ]:\nConfusion Matrix for XGBClassifier (Cross-Validation):\n[[20  3]\n [ 3 20]]\n\nCross-validation accuracy for SVC: 1.0\ntrue pos and fal pos [0. 0. 0. 1.] [0.         0.04347826 1.         1.        ]:\nConfusion Matrix for SVC (Cross-Validation):\n[[23  0]\n [ 0 23]]\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n               Classifier  Accuracy (CV Mean)\n0              GaussianNB            1.000000\n0  DecisionTreeClassifier            0.802222\n0  RandomForestClassifier            1.000000\n0      LogisticRegression            1.000000\n0           MLPClassifier            0.980000\n0           XGBClassifier            0.868889\n0                     SVC            1.000000\n\n\n5.10 - Plot Accuracy data\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy (CV Mean)', y='Classifier', data=log, color=\"b\")\n\nplt.xlabel('Accuracy %')\nplt.title('Classifier Accuracy Normal Models')\nplt.show()\n\n\n\n\n\n\n\n\n5.10 - Evaluate Model Performance - Some models performed reasonably well/ Was not expecting this given the limited data size. - MLPClassifier gives 1.0 ROC curve value and an accuracy of .925 in both pre and post SMOTE Data. This rings alarms bells that data is overfitting, or something is not right. This would require more investigation, would discount this model data. - Best performing models - LogisticRegression - largest ROC AUC value at .95 but has an accuracy of .75 which is 5th best accuracy. Performed well. - XGBClassifier - 0.81 ROC AUC value abut has an accuracy of 0.875. Again performed well. - Other also performed reasonably well.\n\nLogisticRegression had the followinfg confusion matrix.\n\n[[ 0  7]\n [ 0 23]]\n\n\n=&gt; 7 False negatives - said 'bad' esg but was 'good' esg company\n=&gt; 23 True Positives - said 'good' esg and was 'good' esg company\n=&gt; 4 True Negatives - said 'bad' esg and was 'bad' esg company\n=&gt; 0 False Positive - said 'good' esg and was 'bad' esg company\n\nXGBClassifier had the following confusion matrix\n\n[[ 4  3]\n [ 1 22]]\n\n=&gt; 3 False negatives - said 'bad' esg but was 'good' esg company\n=&gt; 22 True Positives - said 'good' esg and was 'good' esg company\n=&gt; 4 True Negatives - said 'bad' esg and was 'bad' esg company\n=&gt; 0 False Positive - said 'good' esg and was 'bad' esg company\nFewer False negatives is important as means telling people the company has poor ethical esg values when infact it has good esg values. This could potentially be unfair and lost business to the companies who were incorrectly labelled. This would need further investigation to try minimise the False negative values.\nPost-SMOTE applied data did make a difference. SVC has an ROC Value of .99, which again rings alarm bells and means this along with MLPClassifier would both be discounted as there is an issue with these being 1.0 or close to 1.0.\nXGBClassifier and LogisticRegression remained stable and similar to the use of pre-smote data, whereas DecisionTree and RandomForest both deteriorated with SMOTE data.\nBest model overall would possibly be a combination of LogicalRegression and XGBClassifier. These both performed well and were stable in both scenarios.\nOverall, a larger data set is still required to validate the data and performance and further investigation and validation is required.\n\nStep 6 - Conclusions and Learnings\nCan we predict whether a company has good or bad esg practices based on reviewing their company reports. This is indicated by\n\n1 = good ‘esg’ company\n0 = bad ‘esg’ company\n\nSomewhat surprisingly, two elements wold lead us to believe that this shows promise in text mining analysis and classification as a method to indicate ‘esg’ company performance 1. Part 1. Data Analysis - The graph Positive Ethical Keyword Count per Company per 1000 words matches our expectations. - Least Ethical with Lowest Counts per 1000 words = exxonmobil, jbs and aramco - These 3 were selected as they have very low esg rating across our esg ratings i.e. bad esg companies - Companies with the Most ethical word counts per 1000 words = kerrygroup, crh, snmufitkappa and kingspan - These 4 were selected due to their high esg ratings i.e. esg good esg companies 1. Part 2. Classification - The cross fold validation and smote correctly correctly predicted 10 and 5 bad companies respectively. This is tghe most positive sign for me and a good indication of potential, especially given our small data set. true negatives woudl be hardest to predict as companies will use positive terms in reports many times, but the model cna still predict bad esg performers despite this.\n\n6.1 Challenges\n\nStandardizing and cleaning the date. Not all reports are the same or standardized. Lots of variations in data provided and formats. Volume of data. Need to get a lot more company report\nTime to process the pdfs is approx between 30 secs and 2 mins per pdf. This adds up when dealing with large numbers of pdfs… It takes approx 40 mins to process our small data set pof pdfs\nCompanies can stack the reports by repeating positive terms. Always a risk, even if our classification models could see through that in a good few instances in our data set\nStorage - we need to increase our dataset hugely across a traneg of sources to improve accuracy. Storage may become an issue as data storage needs increase….\n\nwe used 44 files across 7 companies which took up aprox 500 MB of space\n\n\n\n\n6.2 Improvements\n\ntie it into Company Registration Office (CRO) data to include ownership details\nGet more sample data\nInclude social media and news articles for each company to enhance the data and catch potentisal negativity around it\nInclude court documetn data\nif have different data sources i.e.company reports, social media, news, courts data can weight those accordingly to give more accurate result.\n\nOverall, this has been surprisingly successful. The initial assumption would be that it would be too difficult to discern a companies esg status solely base don their company reports. However, this shows great potential if the data set could be increased and especially if it can eb broadened to include other sources other than self authored reports.\n\n\n\nAppendix - References\n\noralytics &gt;&gt;&gt; #GE2020 Analysing Party Manifestos using Python\nhttps://github.com/pdfminer/pdfminer.six &gt;&gt;&gt; community maintained fork of the original PDFMiner\npdfprimer docs\nhttps://www.nltk.org/\nhttps://docs.python.org/3/howto/regex.html\nhttps://regex101.com/\nGeneral ESG references outside of those already included are :\n\nhttps://www.responsible-investor.com/\nhttps://www.knowesg.com/\nvarious news sites\n\n\n[2] Python – Convert list of dictionaries to JSON"
  }
]